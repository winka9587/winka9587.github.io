---
title: attention is all you need to learn
date: 2024-09-30 10:51
category: Net
author: winka9587
tags: [Net]
summary: Summary of the article ViT in MASt3r
math: true
---

[toc]

å­¦ä¹ å’Œä½¿ç”¨Transformerå¾ˆä¹…äº†, å¾ˆå¤šåœ°æ–¹çš„ç†è§£éƒ½æ¯”è¾ƒæ··ä¹±ï¼Œå› ä¸ºå­¦ä¹ çš„èµ„æ–™å’Œä½¿ç”¨çš„æ¨¡å—åœ¨å¾ˆå¤šéƒ¨åˆ†å·²ç»ä¸åŒæ­¥äº†ã€‚ä¸€ç›´æƒ³æŒ‰ç…§æ—¶é—´é¡ºåºä¸²ä¸€ä¸‹Transformerçš„æ¼”åŒ–è¿‡ç¨‹ï¼Œå°±è¶ç€è¿™ä¸ªæ˜¥èŠ‚çš„æ—¶é—´ï¼Œä¸€ç›´æ¨åˆ°ViTä¸ºæ­¢å§ã€‚

## Scaled Dot-Product Attention

ä»¥ä¸‹å…¬å¼æ¥è‡ª[Attention Is All You Need (2017)](https://arxiv.org/pdf/1706.03762)

1.è¾“å…¥ä¸€ä¸ªåºåˆ—ï¼ŒåŒ…å«nä¸ªå…ƒç´ ï¼Œå°†è¿™nä¸ªwordé€šè¿‡çº¿æ€§å˜æ¢, $W_Q, W_K, W_V$å¾—åˆ°æˆ‘ä»¬çš„ä¸»è§’$Q,K,V$.ä¹Ÿå¤§æ¦‚ç†è§£äº†ä¸ºä»€ä¹ˆå¾ˆå¤šçš„æ•™ç¨‹ä»¥NLPä»»åŠ¡ä¸¾ä¾‹, å› ä¸ºæœ€æ—©çš„æ³¨æ„åŠ›æå‡ºæ¥å°±æ˜¯ä¸ºäº†è§£å†³NLPä»»åŠ¡çš„, åƒæ˜¯ViTç­‰æ˜¯åæ¥æ‰å‡ºç°çš„ã€‚

å¯¹äºQä¸­æ¯ä¸€ä¸ªå‘é‡$q_i$ï¼Œè®¡ç®—ä¸Kä¸­æ‰€æœ‰$k_j$çš„ç›¸ä¼¼æ€§ï¼Œå¯¹åº”åˆ°æ³¨æ„åŠ›çš„åŸºç¡€å…¬å¼, queryä¸keyè®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦, è¿™ä¸€ç‚¹åœ¨feature matchä¸­éƒ½æ˜¯éå¸¸å¸¸è§çš„æ“ä½œäº†ã€‚

$$
\textit{Attention Score}(Q, K) = Q K^T
$$

ä½†æˆ‘ä»¬èƒ½å‘ç°ï¼Œè®ºæ–‡ä¸­çš„æ³¨æ„åŠ›å…¬å¼å¤šä¸€ä¸ªç¼©æ”¾å› å­$\sqrt{d}$ä½œä¸ºåˆ†æ¯

$$
\textit{Scaled Attention Score}(Q, K) = \frac{Q K^T}{\sqrt{d}}
$$

![alt text](/assets/img/scaled_attn.png)
<p style="text-align: center;">From: Attention Is All You Need, Figure 2: (left) Scaled Dot-Product Attention.</p>

MatMul: $QK^T$

Scale: $\frac{1}{\sqrt{d}}$

å¯ä»¥å‘ç°, å…¬å¼ä¸­å¹¶æ²¡æœ‰Maskè¿™ä¸€éƒ¨åˆ†ï¼Œ(opt.)ä¹Ÿè¡¨æ˜å®ƒæ˜¯å¯é€‰çš„ã€‚è¿™éƒ¨åˆ†åœ¨Decoderä¸­çš„self-attnä¸­ä¼šè¢«ä½¿ç”¨åˆ°ã€‚

### ç¼©æ”¾å› å­çš„ä½œç”¨

ä¸€èˆ¬æ¥è¯´, ä¸€ä¸ªç½‘ç»œä¸­çš„ç‰¹å¾ç»´åº¦dæ˜¯ä¸€ä¸ªå›ºå®šå€¼, å› æ­¤ç›¸å½“äºå¯¹Qå’ŒKçš„ç›¸ä¼¼åº¦é™¤ä»¥äº†ä¸€ä¸ªå›ºå®šå€¼, è¿™ä¸ªå€¼æ˜¯éšç€ç½‘ç»œçš„ç‰¹å¾ç»´åº¦è€Œæ”¹å˜çš„, å³$\sqrt{d}$æ˜¯ç»´åº¦ç›¸å…³çš„ç¼©æ”¾å› å­ã€‚å®ƒçš„ä½œç”¨æ˜¯ç¼©å°ç‚¹ç§¯çš„æç«¯å€¼ï¼Œä½¿å¾—æ³¨æ„åŠ›å¾—åˆ†åœ¨ç»è¿‡softmaxä¹‹åè¾“å‡ºçš„ç»“æœæ›´åŠ å¹³ç¼“ã€‚

---

ä¸¾ä¸ªæœ€ç®€å•çš„ä¾‹å­:

å‡è®¾è¾“å…¥ç»è¿‡çº¿æ€§å˜æ¢åå¾—åˆ°:
$$
 q_1 = [1, 0, 1, 0] \\
 q_2 = [0, 1, 0, 1] \\
 k_1 = [1, 0, 1, 0] \\
 k_2 = [0, 1, 0, 1]
$$

è®¡ç®—$Q \cdot K^T$

$$
Q \cdot K^T = \begin{pmatrix}
q_1 \cdot k_1 & q_1 \cdot k_2 \\
q_2 \cdot k_1 & q_2 \cdot k_2 
\end{pmatrix}
= \begin{pmatrix}
2 & 0 \\
0 & 2
\end{pmatrix}
$$

$$
softmax(Q \cdot K^T)=\begin{pmatrix}
0.88 & 0.12 \\
0.12 & 0.88
\end{pmatrix}
$$

å¦‚æœæ·»åŠ äº†ç¼©æ”¾å› å­$\sqrt{d}$

$$
softmax(\frac{Q \cdot K^T}{\sqrt{d}}) =
softmax\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
=\begin{pmatrix}
0.73 & 0.27 \\
0.27 & 0.73
\end{pmatrix}
$$

å¯ä»¥å‘ç°, scaled attention scoreç›¸æ¯”äºä¸å¸¦ç¼©æ”¾å› å­çš„ç‰ˆæœ¬, ç›¸ä¼¼åº¦åˆ†æ•°çš„å·®å¼‚æ›´å°äº†ã€‚é‚£ä¹ˆ$\sqrt{d}$æ˜¯å¦å¯ä»¥æ›¿æ¢ä¸ºå¸¸é‡ï¼Ÿå½“ç„¶å¯ä»¥, ä½†æœ€å¥½è¿˜æ˜¯è·Ÿéšç»´åº¦æ¥è¿›è¡Œå˜åŒ–, å› ä¸ºä¸åŒçš„ä»»åŠ¡ä¸­å‘é‡çš„ç»´åº¦å¯èƒ½å·®å¼‚éå¸¸å¤§, å¦‚æœç¼©æ”¾å› å­è¿‡å¤§æˆ–è¿‡å°, éƒ½ä¸èƒ½å¤Ÿè¾¾åˆ°ç¼©å°æç«¯å€¼çš„æ•ˆæœã€‚

---

ä½¿ç”¨$Q,K$è®¡ç®—ç›¸ä¼¼åº¦å¹¶è¾“å…¥softmaxï¼Œå€ŸåŠ©å…¶éçº¿æ€§ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å’Œå¹³æ»‘å, ä½œä¸ºæ³¨æ„åŠ›æƒé‡, è®¡ç®—ä¸$V$çš„åŠ æƒï¼Œè‡³æ­¤å°±æ˜¯æˆ‘ä»¬æ‰€ç†Ÿæ‚‰çš„æ³¨æ„åŠ›å…¬å¼:

$$
\textit{Attention}(Q, K, V)=softmax(\frac{Q \cdot K^T}{\sqrt{d}})V
$$

å…¶å®ç¼©æ”¾å› å­è¿™ä¸ªä¸œè¥¿æœ¬èº«æ˜¯ä¸ºç‚¹ä¹˜æ³¨æ„åŠ›(dot-product attention)è€ŒæœåŠ¡çš„, æ˜¯ä¸ºäº†å…‹æœç‰¹å¾ç»´åº¦$d$çš„å½±å“ï¼Œæœ‰çš„å…¬å¼ä¸­ä¹Ÿä¼šå¼ºè°ƒ$d$æ˜¯Kçš„ç»´åº¦$d_k$, ä½†æ˜¯å°±ç›®å‰çœ‹åˆ°çš„å¤§éƒ¨åˆ†å®ç°æ¥è¯´, $Q$å’Œ$K$çš„ç»´åº¦$d_q$å’Œ$d_k$é€šå¸¸æ˜¯ç›¸åŒçš„ï¼Œå› ä¸ºä¾¿äºè®¡ç®—ã€‚å› æ­¤ç¼©æ”¾è¿™éƒ¨åˆ†æ˜¯æœ‰ç‚¹åç»éªŒå‘çš„ä¸œè¥¿ã€‚å¯ä»¥å‚è€ƒåŸæ–‡ä¸­çš„æè¿°ï¼š

![](/assets/img/2024-10-08-15-26-16.png)

### æ³¨æ„åŠ›ä¸æ¢¯åº¦æ¶ˆå¤±

~~~
æ³¨æ„åŠ›é€šå¸¸å¯ä»¥è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜, å› ä¸ºå…¶åœ¨encoder statusä¸decoderä¹‹é—´å»ºç«‹äº†ç›´æ¥è¿æ¥ã€‚ä¸CNNä¸­çš„è·³è¿æ¥éå¸¸ç›¸ä¼¼ã€‚
~~~

æ¢¯åº¦æ¶ˆå¤±æ˜¯å› ä¸ºç½‘ç»œè¿‡æ·±æ—¶ï¼Œåå‘ä¼ æ’­è®¡ç®—çš„æ¢¯åº¦å¾ˆå°ï¼Œå¯¼è‡´å‰é¢çš„å±‚éš¾ä»¥æ›´æ–°æƒé‡ã€‚åœ¨ä¼ ç»Ÿçš„RNN, LSTMç­‰åºåˆ—æ¨¡å‹ä¸­éå¸¸å¸¸è§ã€‚

![alt text](/assets/img/transformer.png)
<p style="text-align: center;">From: Attention Is All You Need, Figure 1: The Transformer - model architecture.</p>

è§‚å¯Ÿtransformerçš„ç»“æ„å›¾æˆ‘ä»¬å¯ä»¥å‘ç°ï¼šencoderçš„è¾“å‡ºç›´æ¥è¿æ¥åˆ°decoderçš„MHAçš„è¾“å…¥ã€‚

#### self-attnä¸cross-attn

ç»“æ„å›¾ä¸­, å·¦ä¾§Encoderå’Œå³ä¾§Decoderçš„ç¬¬ä¸€ä¸ªMHAéƒ½æ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶ç”¨äºè®¡ç®—è¾“å…¥åºåˆ—å†…éƒ¨æ³¨æ„åŠ›å’Œå…³ç³»ï¼Œè€ŒDecoderä¸­çš„ç¬¬äºŒä¸ªæ³¨æ„åŠ›ï¼Œè¾“å…¥åˆ†åˆ«æ¥è‡ªè¾“å…¥åºåˆ—å’Œè¾“å‡ºåºåˆ—çš„ç»“æœï¼Œä¸ºcross-attn, è·¨æ¨¡æ€äº¤æ¢ä¿¡æ¯ã€‚

#### Masked MHA

å¯ä»¥æ³¨æ„åˆ°, Decoderçš„ç¬¬ä¸€ä¸ªMHAæ˜¯Masked MHA, å…¶ä¸­çš„maskæ˜¯ä¸ºäº†ä¿æŒ**è‡ªå›å½’(auto-regressive)**ç‰¹æ€§ã€‚

> è§£ç å™¨çš„è‡ªæ³¨æ„åŠ›å¿…é¡»é¿å…æœªæ¥ä¿¡æ¯çš„æ³„éœ²ï¼Œç¡®ä¿æ¨¡å‹åœ¨ç”Ÿæˆç¬¬ ğ‘¡ ä¸ª token æ—¶ï¼Œåªèƒ½ä¾èµ– ğ‘¡ ä¹‹å‰çš„ä¿¡æ¯ï¼Œè€Œä¸èƒ½æå‰çœ‹åˆ°æœªæ¥çš„ tokenã€‚

å¦‚ä½•ç†è§£ï¼Ÿ

å…¶å®åº”è¯¥è¿™ä¹ˆç†è§£: ä»¥ è¯­è¨€ç¿»è¯‘ ä¸ºä¾‹, 

è¾“å…¥ï¼ˆEnglishï¼‰: I love apple
ç›®æ ‡ï¼ˆChineseï¼‰: æˆ‘ å–œæ¬¢ è‹¹æœ

Decoderä¸­çš„ç¬¬ä¸€ä¸ªMask MHAå…¶å®æ˜¯åœ¨å­¦ä¹ ç›®æ ‡è¯­è¨€ä¹‹é—´çš„è¯­æ³•ç­‰å…³ç³»ï¼Œè€Œç¬¬äºŒMHAï¼ˆcross-attnï¼‰æ‰æ˜¯å­¦ä¹ ä¸¤ç§è¯­è¨€ä¹‹é—´çš„æ˜ å°„ï¼Œå› æ­¤ï¼Œåœ¨Decoderçš„ç¬¬ä¸€ä¸ªMask MHAä¸­, "æˆ‘"ä¸åº”è¯¥èƒ½çœ‹åˆ°"å–œæ¬¢"å’Œ"è‹¹æœ", "å–œæ¬¢"ä¸èƒ½çœ‹åˆ°"è‹¹æœ", å¦åˆ™å°±ç›¸å½“äºæå‰çœ‹åˆ°äº†ç­”æ¡ˆ, å˜æˆäº†å¯¹æ•°æ®é›†çš„æ‹Ÿåˆã€‚

é‚£ä¹ˆè¿˜å‰©ä¸‹ä¸€ä¸ªé—®é¢˜, åœ¨Mask MHAä¸­, maskæ˜¯å¦‚ä½•æ·»åŠ çš„ï¼Ÿï¼ˆå…·ä½“æ“ä½œè€Œè¨€ï¼‰

å®ç°èµ·æ¥å°±æ›´ç®€å•äº†ï¼Œç›´æ¥åœ¨è®¡ç®—ç”¨äºåŠ æƒçš„ç›¸ä¼¼åº¦å¾—åˆ†æ—¶ï¼Œ å¦‚æœå¯¹åº”çš„å…ƒç´ æ˜¯æ¥è‡ªæœªæ¥çš„, ç›´æ¥å°†å…¶ç½®ä¸º0, ä¸å…è®¸å…¶å‚ä¸è®¡ç®—å³å¯ã€‚

ä¾‹å¦‚:

Encoderä¸­:

$$
\text{Output}_{I}=\alpha_{I, I}V_{I} + \alpha_{I, love}V_{love} + \alpha_{I, apple}V_{apple}
$$

Decoderçš„mask self-attnä¸­:

$$
\text{Output}_{æˆ‘}=\alpha_{æˆ‘, æˆ‘}V_{æˆ‘} + \alpha_{æˆ‘, å–œæ¬¢}V_{å–œæ¬¢} + \alpha_{æˆ‘, è‹¹æœ}V_{è‹¹æœ}
$$

$$
\alpha_{æˆ‘, å–œæ¬¢}V_{å–œæ¬¢} = 0, \alpha_{æˆ‘, è‹¹æœ} = 0
$$

ä¸å…è®¸æœªæ¥çš„å…ƒç´ å‚ä¸è®¡ç®—ã€‚

## MHA(Multi-Head Attention)

Multi-headæ˜¯å¦‚ä½•å‡ºç°çš„?

æ ¹æ®ä¸Šè¿°çš„æ³¨æ„åŠ›å…¬å¼, è¾“å…¥

å‡è®¾è¾“å…¥åºåˆ—çš„é•¿åº¦ä¸º$n$, çŸ©é˜µ$Q, K, V$çš„shapeåˆ†åˆ«ä¸º$\mathbb{R}^{n \times d_q}$, $\mathbb{R}^{n \times d_k}$, $\mathbb{R}^{n \times d_v}$

æœ€ç»ˆå¾—åˆ°çš„åŠ æƒå’Œç»´åº¦ä¹Ÿæ˜¯$\mathbb{R}^{n \times d_v}$

ä¸¾å…·ä½“ä¾‹å­, 

**è¾“å…¥**: 

é•¿åº¦ä¸º$n$çš„åºåˆ—, å¾—åˆ°shapeåˆ†åˆ«ä¸º$\R^{n \times d_q}$, $\R^{n \times d_k}$, $\R^{n \times d_v}$çš„çŸ©é˜µ$Q, K, V$(å› ä¸ºQå’ŒKè¦è¿›è¡Œç‚¹ç§¯æ“ä½œ, æ‰€ä»¥è¦æ»¡è¶³$d_q=d_k$, ä½†å¾ˆå¤šæƒ…å†µä¸‹, $d_v$ä¸$d_q, d_k$ä¹Ÿæ˜¯ç›¸åŒçš„)
æœ€ç»ˆå¾—åˆ°çš„åŠ æƒå’Œè®¡ç®—å¾—åˆ°çš„åŠ æƒå’Œç»´åº¦ä¹Ÿæ˜¯$\R^{n \times d_v}$

**Q:å¤šå¤´æ³¨æ„åŠ›æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ**

A:åˆ†åˆ«ä½¿ç”¨$h$ä¸ªç‹¬ç«‹çš„çº¿æ€§å˜æ¢$W_i(i=1, 2, ..., h)$
å°†$Q, K, V$è¿›è¡Œçº¿æ€§å˜æ¢ã€‚

è‹¥æœ‰$h$ä¸ªå¤´, æ¯ä¸ªå¤´çš„ç»´åº¦æ˜¯$d_h$
$$
d_h = \frac{d}{h}
$$

å¯¹è¿›è¡Œçº¿æ€§å˜æ¢
$$
Q' = QW_h
$$

$$
(n, hd_h) \larr (n, d_q) Ã— (d_q, hd_h) \larr (n, d_q)Ã—(d_q, d)
$$

å¯¹$K, V$è¿›è¡ŒåŒæ ·çš„å¤„ç†

ä¹‹åæœ‰ä¸€ä¸ªé˜¶æ®µ, åˆ’åˆ†å¤´

$d$ç§°ä¸ºå¤šå¤´æ³¨æ„åŠ›çš„åµŒå…¥ç»´åº¦(representation dimensionality)(å¤šå¤´æ³¨æ„åŠ›æ€»ç»´åº¦), $h$æ˜¯å¤´çš„æ•°é‡, 

$$
Q_h = \text{reshape}(Q', (h, n, d_h))
$$

åˆ’åˆ†åæ¯ä¸ªå¤´æœ‰ä¸€ä¸ªç‹¬ç«‹çš„key, query, valueé›†åˆ, åœ¨å•ç‹¬çš„å­ç©ºé—´ä¸­è®¡ç®—ã€‚

$$
\text{Attention}_i = softmax(\frac{Q^i_h \cdot {K^i_h}^T}{\sqrt{d_h}})V^i_h, i=1,...,h
$$

$h$ä¸ªå¤´, æ¯ä¸ªå¤´è¾“å‡ºçš„ç»´åº¦æ˜¯$(n, d_h)$

å°†$h$ä¸ªå¤´çš„è¾“å‡ºconcat, å¾—åˆ°$(n, hd_h)$ç»´åº¦çš„è¾“å‡ºã€‚æ‹¼æ¥åç»“æœç»è¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢å¾—åˆ°æœ€ç»ˆè¾“å‡ºã€‚

$$
O = \text{concat}(O_1, O_2, ..., O_h)
$$

æ•´ä¸ªè¿‡ç¨‹å¯ä»¥è¢«ç®€åŒ–ï¼š

$Q'=QW_h$å’Œ$Q=XW_q$ä¸¤æ­¥æ“ä½œå¯ä»¥åˆå¹¶ä¸ºä¸€æ­¥$Q'=XW^Q_h$ï¼Œä¸€èˆ¬ä¼šå†™æˆ$Q_h=XW^Q_h$ï¼Œä½†æ˜¯ä»$Q'$åˆ°$Q_h$å…¶å®è¿˜å·®äº†ä¸€æ­¥reshapeæ“ä½œ
$Q_h=\text{reshape}(Q')$

$$
Q'=XW_qW_h
$$

$$
(n, hd) \larr (n, c) Ã— (c, d_q) Ã— (d_q, hd_h)
$$

$$
Q_h = \text{reshape}(Q', (h, n, d_h))
$$

ä¸€èˆ¬æƒ…å†µä¸‹, æœ€ç»ˆreshapeçš„$(B, H, N, d_k)$å®ç°æ˜¯æ›´å¸¸è§çš„ã€‚

### å¾—åˆ°å¤šå¤´è¾“å‡ºå

é¦–å…ˆè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°

$$
A_h=softmax(\frac{Q_h \cdot K_h^T}{\sqrt{d}})
$$

$Q_h: (B, H, N, d_h)$

$K_h^T: (B, H, d_h, N)$ äº¤æ¢æœ€åä¸¤ä¸ªç»´åº¦

æœ€ç»ˆå¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°$A_h$çš„shapeä¸º$(B, H, N, N)$

åŠ æƒè®¡ç®—åå¾—åˆ°$O_h:(B, H, N, d_h)$

$$
O_h = A_hV_h
$$

æ‹¼æ¥å¤šå¤´è¾“å‡º

$$
O = \text{concat}(O_1, O_2, ..., O_H)
$$

æ‹¼æ¥è¿™ä¸€æ­¥é€šå¸¸æ˜¯ä¸€ä¸ªç»´åº¦äº¤æ¢+reshapeå®ç°çš„

$\text{permute}(0, 2, 1, 3):(B, H, N, d_h) \rarr (B, N, H, d_h)$

$\text{reshape}(B, N, hd_h):(B, N, H, d_h) \rarr (B, N, hd_h) \rarr (B, N, d)$

ç»¼ä¸Šå¯ä»¥å‘ç°, å°†å¤šå¤´çš„è¾“å‡ºç»è¿‡permuteæ“ä½œå˜ä¸º$(B, H, N, d_h)$æ˜¯ä¸ºäº†æ–¹ä¾¿å…ˆè¿›è¡Œæ³¨æ„åŠ›åˆ†æ•°è®¡ç®—, ç­‰æ‹¼æ¥æ—¶å†å»reshapeã€‚

![alt text](/assets/img/MHA.png)
<p style="text-align: center;">From: Attention Is All You Need, Figure 2: (right) Multi-Head Attention consists of several attention layers running in parallel.</p>

---

### ä¸ºä»€ä¹ˆè¦ä½¿ç”¨å¤šå¤´æœºåˆ¶ï¼Ÿ

1. **å¹¶è¡Œè®¡ç®—** é€šè¿‡MHAçš„ç»“æ„å›¾å¾ˆå®¹æ˜“å‘ç°, ä¸åŒçš„headæ³¨æ„åŠ›è®¡ç®—æ˜¯å¯ä»¥å¹¶è¡Œçš„ã€‚

2. **èƒ½å¤Ÿæ˜ å°„ç‰¹å¾åˆ°æ›´å¤šçš„å­ç©ºé—´** ä½¿æ¨¡å‹ä»ä¸åŒçš„è§†è§’å­¦ä¹ ç‰¹å¾æ¥å¢å¼ºè¡¨è¾¾èƒ½åŠ›ï¼Œè¿™ä¸ªæ‰€è°“çš„ä¸åŒè§†è§’ï¼Œæ˜¯"hä¸ªå¤´ä½¿ç”¨ç‹¬ç«‹çš„çº¿æ€§å˜æ¢"å®ç°çš„, å³ hä¸ªå¤´åœ¨æ˜ å°„QKVæ—¶ä½¿ç”¨çš„$W_q, W_k, W_v$æ˜¯ä¸åŒçš„, ä»¥ä¿è¯å…¶è®¡ç®—çš„æ¢¯åº¦ä¸åŒ$\rarr$ä¼˜åŒ–çš„æ–¹å‘ä¸åŒ$\rarr$å„ä¸ªå¤´æ˜ å°„åˆ°çš„å­ç©ºé—´ä¸åŒ

---

æ ¹æ®è®ºæ–‡ä¸­çš„æè¿°:

é¦–å…ˆ: å°†qkvçš„å€¼é€šè¿‡$h$æ¬¡ä¸åŒçš„å­¦ä¹ åˆ°çš„çº¿æ€§å˜æ¢å¾—åˆ°$d_k, d_k, d_v$ç»´çš„è¾“å…¥, æ¯”ç›´æ¥ä½¿ç”¨ç›¸åŒçš„$d_{model}$ç»´çš„è¾“å…¥æ•ˆæœè¦å¥½ã€‚åœ¨æ¯ä¸€æ¬¡çº¿æ€§æŠ•å½±åçš„ç»“æœä¸Šè®¡ç®—æ³¨æ„åŠ›

---

## Transformer

åˆ†æå®Œäº†attention, æˆ‘ä»¬å›åˆ°æœ€å¼€å§‹transformerçš„ç»“æ„å›¾

![alt text](/assets/img/transformer.png)
<p style="text-align: center;">From: Attention Is All You Need, Figure 1: The Transformer - model architecture.</p>

ä»¥NLPä¸¾ä¾‹, è¾“å…¥"I am human"

**Input Embedding** å°†ä¸‰ä¸ªåŸå§‹è¾“å…¥é€šè¿‡åµŒå…¥å±‚(Embedding Layer)è½¬æ¢ä¸ºæŒ‡å®šç»´åº¦(d=512)çš„å‘é‡(3, 512)

**Positional Encoding** ä½ç½®ç¼–ç ä¸ºæ¯ä¸ªåµŒå…¥å‘é‡æ·»åŠ ä½ç½®ç¼–ç ã€‚ä¾‹å¦‚$X_i=E_i+{PE}_i, $ $E_i \in \mathbb{R}^{d_{model}}$ä¸ºEmbeddingåæå–çš„å‘é‡, ${PE}_i \in \mathbb{R}^{d_{model}}$ä¸ºä½ç½®ç¼–ç ã€‚*(éœ€è¦ä½ç½®ç¼–ç æ˜¯å› ä¸ºTransformerä¸åƒRNNå’ŒLSTMç­‰æ¨¡å‹ä¸€æ ·æœ‰éšå‘é‡å’Œé€’å½’ç»“æ„ã€‚é€’å½’æœ¬èº«å°±éšå«äº†ä½ç½®/é¡ºåºå…³ç³»)*

**MHA** self-attentionè®¡ç®—, "I", "am", "human"åœ¨æ¯ä¸ªhead, ä¼šé€šè¿‡å„è‡ªç‹¬ç«‹çš„$W^Q, W^K, W^V$è®¡ç®—å¾—åˆ°å„è‡ªçš„ä¸‰ä¸ªè¡¨ç¤º$Q, K, V$, ä¸åŒå‘é‡ä¹‹é—´äº’ç›¸è®¡ç®—å…³ç³»æƒé‡ä»¥ç†è§£ä¸åŒå‘é‡ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚

æ¯ä¸ªå‘é‡å¯¹åº”çš„è¾“å…¥æ˜¯å…¶ä¸å…¶ä»–å‘é‡ï¼ˆåŒ…æ‹¬è‡ªèº«ï¼‰çš„æ³¨æ„åŠ›æƒé‡åŠ æƒåçš„ç»“æœã€‚

$$
\text{Output}_{I}=\alpha_{I, I}V_{I} + \alpha_{I, am}V_{am} + \alpha_{I, human}V_{human}
$$

**Feed Forward** Fully-connected feed-forward network, å¯¹æ¯ä¸€ä¸ªä½ç½®(è¯)è¿›è¡Œç‹¬ç«‹çš„çº¿æ€§å˜æ¢ã€‚æ ¹æ®è®ºæ–‡ä¸­çš„æè¿°æ˜¯**çº¿æ€§å˜æ¢($512 \rarr 2048$)+ReLU(æ¿€æ´»å±‚)+çº¿æ€§å˜æ¢($204 8\rarr 512$)**

$$
FFN(x)=max(0, xW_1+b_1)W_2 + b_2
$$

å…ˆæ˜ å°„åˆ°æ›´é«˜ç»´åº¦, é€šè¿‡æ¿€æ´»å±‚æ¿€æ´»ï¼Œç„¶åæ˜ å°„å›åŸæœ¬ç»´åº¦ã€‚ä¸€æ˜¯ä¸ºäº†æ˜ å°„åˆ°æ›´é«˜ç»´åº¦å¢å¼ºè¡¨ç¤ºèƒ½åŠ›ï¼ŒäºŒæ˜¯é€šè¿‡æ¿€æ´»å±‚å¼•å…¥æ¨¡å‹çš„éçº¿æ€§å˜æ¢ä»¥æ•è·æ›´å¤æ‚çš„è¾“å…¥æ¨¡å¼ã€‚

**Add&Norm** æ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–ä¸»è¦æ˜¯ä¸ºäº†é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±/æ¢¯åº¦çˆ†ç‚¸å’Œç»´æŒæ•°å€¼ç¨³å®šæ€§ã€‚

### Decoderéƒ¨åˆ†

åœ¨åˆæ­¥å­¦ä¹ çš„æ—¶å€™, å¾ˆå®¹æ˜“å°†transformerçš„ç»“æ„ä¸ä¼ ç»Ÿçš„seq2seqåˆ’ä¸Šç­‰å·ã€‚å®é™…ä¸Šï¼Œtransformerçš„Encoderå’ŒDecoderä¸æ˜¯ç±»ä¼¼äº"$A \rightarrow \text{Encoder} \rightarrow B, B \rightarrow \text{Decoder} \rightarrow A$"çš„ç»“æ„ã€‚

è¿˜æ˜¯ä»¥"æˆ‘æ˜¯äººç±»"å’Œ"I am human"çš„ç¿»è¯‘ä¸ºä¾‹å­

I, am, human æ‹†åˆ†ä¸ºwordå…ƒç´ , åœ¨ç¿»è¯‘ä»»åŠ¡ä¸­, ä¼šå®ç°åˆ›å»ºå¥½è¯è¡¨Vocabulary, è¯è¡¨è´Ÿè´£æ˜ å°„ **å•è¯** å’Œ **ä¸‹æ ‡**, ä¸‹æ ‡ ç”¨äºå»EmbeddingçŸ©é˜µä¸­è¯»å–å¯¹åº”çš„**dç»´å‘é‡**ï¼ˆå½“ç„¶ç°ä»£çš„NLPæ¨¡å‹ä¸ºäº†é˜²æ­¢EmbeddingçŸ©é˜µå˜å¾—å·¨å¤§, å¾€å¾€ä¼šä½¿ç”¨å­è¯ã€å­—èŠ‚å¯¹ç¼–ç BPEç­‰æ–¹å¼æ¥å‹ç¼©ï¼Œåœ¨è¿™é‡Œå°±ä¸æ·±å…¥äº†ï¼‰

ç»è¿‡Embeddingå±‚ä¹‹å, å„ä¸ªè¯å¯¹åº”çš„å‘é‡åœ¨é™„åŠ ä½ç½®ç¼–ç ï¼Œç„¶åè¾“å…¥åˆ°MHAä¸­, æ ¹æ®å¤šå¤´æ³¨æ„åŠ›å¾—åˆ†åŠ æƒæ³¨æ„åŠ›æ±‚å’Œ, å¾—åˆ°å„ä¸ªè¯åŠ æƒåå¯¹åº”çš„å‘é‡ã€‚

### Transformerä¸ºä»€ä¹ˆæ²¡æœ‰å¦‚RNNä¸­çš„biasï¼Ÿ

é—®å‡ºè¿™ä¸ªé—®é¢˜ï¼Œå…¶å®æ˜¯æ²¡æœ‰ç†è§£Transformerçš„QKVæ˜¯ä¸ºäº†åšä»€ä¹ˆã€‚

åœ¨RNNä¸­, æˆ‘ä»¬ä½¿ç”¨å¦‚ä¸‹çš„å…¬å¼:

$$
h_t = \text{Activation}(W_x x + W_h h_{t-1}+b)
$$

Activationæ˜¯ä¸€ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°, æ¯”å¦‚tanhæˆ–è€…ReLU. ä¹‹æ‰€ä»¥ç®¡$h_t$å«éšè—çŠ¶æ€(Hidden State)ï¼Œæ˜¯å› ä¸ºå®ƒæ˜¯ä¸€ä¸ªä¸­é—´ç»“æœï¼Œæ—¢ä¸æ˜¯ç½‘ç»œçš„è¾“å…¥ä¹Ÿä¸æ˜¯ç½‘ç»œçš„è¾“å‡ºï¼Œæ˜¯ç½‘ç»œæ•´ä¸ªé»‘ç›’ä¸­çš„ä¸€éƒ¨åˆ†ã€‚

ä¹‹æ‰€ä»¥RNNä¼šæœ‰åç½®é¡¹$b$, æ˜¯ä¸ºäº†é˜²æ­¢é›¶è¾“å…¥å¯¼è‡´çš„è¾“å‡ºç›¸åŒã€‚

### dust3rä¸­çš„qkv_biaså‚æ•°

qkv_bias

## æ‰‹æ’•Transformer


## Crocoä¸­çš„attention

~~~
class Attention(nn.Module):
    def __init__(self, dim, rope=None, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        self.rope = rope 

    def forward(self, x, xpos):
        B, N, C = x.shape

        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).transpose(1,3)
        q, k, v = [qkv[:,:,i] for i in range(3)]
        # q,k,v = qkv.unbind(2)  # make torchscript happy (cannot use tensor as tuple)
               
        if self.rope is not None:
            q = self.rope(q, xpos)
            k = self.rope(k, xpos)
               
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
~~~

### Residual Connection

æ®‹å·®è¿æ¥æ˜¯Transformeræ¶æ„ä¸­éå¸¸å¸¸è§çš„æŠ€æœ¯ã€‚å°†è¾“å…¥ç»“æœé€šè¿‡ä¸€ä¸ªè·³è·ƒè¿æ¥(skip connection, æœ€æ—©ç”±ResNetæå‡º)è¿æ¥åˆ°å­å±‚çš„è¾“å‡ºä¸Šã€‚

$$Output = Layer(x) + x$$

æˆ‘ä»¬ä¹‹å‰çœ‹è¿‡çš„ä¸è®ºæ˜¯self-attnè¿˜æ˜¯cross-attnçš„Transformeréƒ½èƒ½çœ‹åˆ°å®ƒçš„èº«å½±ã€‚

~~~
    # self-attn + MLP
    def forward(self, x, xpos):
        x = x + self.drop_path(self.attn(self.norm1(x), xpos))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x

    # self-attn + cross-attn + MLP
    def forward(self, x, y, xpos, ypos):
        x = x + self.drop_path(self.attn(self.norm1(x), xpos))
        y_ = self.norm_y(y)
        x = x + self.drop_path(self.cross_attn(self.norm2(x), y_, y_, xpos, ypos))
        x = x + self.drop_path(self.mlp(self.norm3(x)))
        return x, y
~~~

åœ¨æœ€æ—©çš„Transformeræ¶æ„ä¸­æˆ‘ä»¬å¯ä»¥å‘ç°, Add(ä¹Ÿå°±æ˜¯Residual Connection)å’ŒNormæ˜¯ä¸€èµ·ä½¿ç”¨çš„ã€‚ä¸»è¦å‡ºç°ä¸¤éƒ¨åˆ†è¾“å‡ºä¹‹åï¼šå¤šå¤´æ³¨æ„åŠ›(Multi-Head Attention, MHA)å’Œå‰é¦ˆç¥ç»ç½‘ç»œ(Feed-Forward Neural Network), è¿™ä¸¤ä¸ªæ®‹å·®è¿æ¥ä¼šå’Œå½’ä¸€åŒ–ä¸€èµ·ä½¿ç”¨(Layer Normalization), ä½¿å¾—Transformerèƒ½å¤Ÿæ›´æ·±åœ°å †å å¤šä¸ªencoderå’Œdecoder, è€Œä¸ä¼šå‡ºç°æ¢¯åº¦æ¶ˆå¤±æˆ–ä¿¡æ¯ä¸¢å¤±ã€‚

![](/assets/img/2024-10-10-18-50-49.png)

## å…¶ä»–

åœ¨å­¦ä¹ Transformeræ—¶å¯èƒ½ä¼šé‡åˆ°\<bos\>, \<eos\>è¿™ç§ç¬¦å·ï¼Œè¿™æ˜¯åœ¨NLPä¸­ä¼šå‡ºç°çš„ï¼Œåœ¨ViTä¸­å¾ˆå°‘é‡åˆ°ã€‚